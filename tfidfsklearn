import pandas as pd 
import numpy as np

DATABERITA = pd.read_csv("Text_Preprocessing.csv", usecols=["label", "title_tokens"])
DATABERITA.columns = ["label", "title"]

DATABERITA.head()

# join list of token as single document string
import ast

def join_text_list(texts):
    texts = ast.literal_eval(texts)
    return ' '.join([text for text in texts])
DATABERITA["title_join"] = DATABERITA["title"].apply(join_text_list)

DATABERITA["title_join"].head()

from sklearn.feature_extraction.text import TfidfVectorizer

# banyaknya term yang akan digunakan, 
# di pilih berdasarkan top max_features 
# yang diurutkan berdasarkan term frequency seluruh corpus
max_features = 1000

# Feature Engineering 
print ("------- TF-IDF on Title data -------")

tf_idf = TfidfVectorizer(max_features=max_features, binary=True)
tfidf_mat = tf_idf.fit_transform(DATABERITA["title_join"]).toarray()

print("TF-IDF ", type(tfidf_mat), tfidf_mat.shape)

terms = tf_idf.get_feature_names_out()

# sum tfidf frequency of each term through documents
sums = tfidf_mat.sum(axis=0)

# connecting term to its sums frequency
data = []
for col, term in enumerate(terms):
    data.append((term, sums[col] ))

ranking = pd.DataFrame(data, columns=['term','rank'])
ranking.sort_values('rank', ascending=False)

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import normalize

max_features = 1000

# calc TF vector
cvect = CountVectorizer(max_features=max_features)
TF_vector = cvect.fit_transform(DATABERITA["title_join"])

# normalize TF vector
normalized_TF_vector = normalize(TF_vector, norm='l1', axis=1)

# calc IDF
tfidf = TfidfVectorizer(max_features=max_features, smooth_idf=False)
tfs = tfidf.fit_transform(DATABERITA["title_join"])
IDF_vector = tfidf.idf_

# hitung TF x IDF sehingga dihasilkan TFIDF matrix / vector
tfidf_mat = normalized_TF_vector.multiply(IDF_vector).toarray()

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import normalize

max_features = 1000

# ngram_range (1, 3) to use unigram, bigram, trigram
cvect = CountVectorizer(max_features=max_features, ngram_range=(1,3))
counts = cvect.fit_transform(DATABERITA["title_join"])

normalized_counts = normalize(counts, norm='l1', axis=1)

tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1,3), smooth_idf=False)
tfs = tfidf.fit_transform(DATABERITA["title_join"])

tfidf_mat = normalized_counts.multiply(tfidf.idf_).toarray()

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import normalize

max_features = 1000


def generate_tfidf_mat(min_gram, max_gram):
    cvect = CountVectorizer(max_features=max_features, ngram_range=(min_gram, max_gram))
    counts = cvect.fit_transform(DATABERITA["title_join"])

    normalized_counts = normalize(counts, norm='l1', axis=1)

    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(min_gram, max_gram), smooth_idf=False)
    tfs = tfidf.fit_transform(DATABERITA["title_join"])

    tfidf_mat = normalized_counts.multiply(tfidf.idf_).toarray()
    
    TF = normalized_counts.toarray()
    IDF = tfidf.idf_
    TF_IDF = tfidf_mat
    return TF, IDF, TF_IDF, tfidf.get_feature_names_out()

# ngram_range (1, 1) to use unigram only
tf_mat_unigram, idf_mat_unigram, tfidf_mat_unigram, terms_unigram = generate_tfidf_mat(1,1)

# ngram_range (2, 2) to use bigram only
tf_mat_bigram, idf_mat_bigram, tfidf_mat_bigram, terms_bigram = generate_tfidf_mat(2,2)

# ngram_range (3, 3) to use trigram only
tf_mat_trigram, idf_mat_trigram, tfidf_mat_trigram, terms_trigram = generate_tfidf_mat(3,3)

#unigram
idx_sample = 1

print("Show TFIDF sample ke-" + str(idx_sample), "\n")
print(DATABERITA["title"][idx_sample], "\n")

print("\t\t\t", "TF", "\t\t", "IDF", "\t\t", "TF-IDF", "\t", "Term\n")
for i, item in enumerate(zip(tf_mat_unigram[idx_sample], idf_mat_unigram, tfidf_mat_unigram[idx_sample], terms_unigram)):
    if(item[2] != 0.0):
        print ("array position " + str(i) + "\t", 
               "%.6f" % item[0], "\t", 
               "%.6f" % item[1], "\t", 
               "%.6f" % item[2], "\t", 
               item[3])
			   
			   
#bigram
idx_sample = 1

print("Show TFIDF sample ke-" + str(idx_sample), "\n")
print(DATABERITA["title"][idx_sample], "\n")

print("\t\t\t", "TF", "\t\t", "IDF", "\t\t", "TF-IDF", "\t", "Term\n")
for i, item in enumerate(zip(tf_mat_bigram[idx_sample], idf_mat_bigram, tfidf_mat_bigram[idx_sample], terms_bigram)):
    if(item[2] != 0.0):
        print ("array position " + str(i) + "\t", 
               "%.6f" % item[0], "\t", 
               "%.6f" % item[1], "\t", 
               "%.6f" % item[2], "\t", 
               item[3])
			   

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
wcss = []
for i in range(1,11):
    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)
    kmeans.fit(tfs)
    wcss.append(kmeans.inertia_)
plt.plot(range(1,11),wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.savefig('elbow.png')

kmeans = KMeans(n_clusters = 8, init = "k-means++", random_state = 42)
kmeans.fit(tfs)
plt.plot(range(1, 11), wcss)
plt.xlabel('Number of clusters')
y_kmeans = kmeans.fit_predict(tfs)
plt.ylabel('WCSS')
plt.show()

from sklearn.decomposition import PCA

# Reduce dimensionality to 2D for visualization
pca = PCA(n_components=2)
reduced_tfs = pca.fit_transform(tfs)

# Plot the clusters in the reduced 2D space
plt.scatter(reduced_tfs[y_kmeans == 0, 0], reduced_tfs[y_kmeans == 0, 1], s=60, c='red', label='Cluster1')
plt.scatter(reduced_tfs[y_kmeans == 1, 0], reduced_tfs[y_kmeans == 1, 1], s=60, c='blue', label='Cluster2')
plt.scatter(reduced_tfs[y_kmeans == 2, 0], reduced_tfs[y_kmeans == 2, 1], s=60, c='green', label='Cluster3')
plt.scatter(reduced_tfs[y_kmeans == 3, 0], reduced_tfs[y_kmeans == 3, 1], s=60, c='violet', label='Cluster4')
plt.scatter(reduced_tfs[y_kmeans == 4, 0], reduced_tfs[y_kmeans == 4, 1], s=60, c='yellow', label='Cluster5') 
plt.scatter(reduced_tfs[y_kmeans == 5, 0], reduced_tfs[y_kmeans == 5, 1], s=60, c='orange', label='Cluster6')
plt.scatter(reduced_tfs[y_kmeans == 6, 0], reduced_tfs[y_kmeans == 6, 1], s=60, c='brown', label='Cluster7') 
plt.scatter(reduced_tfs[y_kmeans == 7, 0], reduced_tfs[y_kmeans == 7, 1], s=60, c='cyan', label='Cluster8')   

# Plot the cluster centroids
plt.scatter(pca.transform(kmeans.cluster_centers_)[:, 0], pca.transform(kmeans.cluster_centers_)[:, 1], 
            s=100, c='black', label='Centroids')

plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()
plt.title('K-Means Clustering (PCA-Reduced Data)')

plt.show()

# Add the cluster labels to the DataFrame
DATABERITA['Cluster'] = y_kmeans

# Select the columns you want to export (e.g., original text, joined text, and cluster labels)
export_df = DATABERITA[['title', 'title_join', 'Cluster']]

# Export the DataFrame to an Excel file
export_df.to_excel('clustered_data.xlsx', index=False)

print("Cluster data has been successfully exported to 'clustered_data.xlsx'")
